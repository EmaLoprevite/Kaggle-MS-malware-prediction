#!/usr/bin/env python
# coding: utf-8

# In[1]:


NB = "TRAINING"


# In[2]:


get_ipython().run_line_magic('run', 'common.ipynb')


# In[3]:


# Create a custom pipeline
from sklearn.pipeline import Pipeline, FeatureUnion

#from sklearn.compose import ColumnTransformer

# Data cleaning for NaN values
from sklearn.impute import SimpleImputer

# Min-max scaling, aka normalisation
# bounded to a range but more affected
# by outliers than standardisation
#from sklearn.preprocessing import MinMaxScaler
# Standardisation
from sklearn.preprocessing import StandardScaler

# One-hot encoding for text attributes
from sklearn.preprocessing import OneHotEncoder

# Bin and encode numerical attributes, NaN not allowed
from sklearn.preprocessing import KBinsDiscretizer

# KBinsDiscretizer might produce constant features
# e.g. encode='onehot' and certain bins do not contain any data
# which can be removed with feature selection algorithms
from sklearn.feature_selection import VarianceThreshold


num_pipeline = Pipeline([
    ('selector', DataFrameSelector(name_col_num)),
    ('imputer', SimpleImputer(strategy='constant', fill_value=-999)),
    #('selector_fix_num', RareNumericValues(name_col_num)),
    #('standard_scaler', StandardScaler()),
])

num_bool_pipeline = Pipeline([
    ('selector', DataFrameSelector(name_col_num_bool)),
    ('imputer', SimpleImputer(strategy='most_frequent')),
])

num_bin_pipeline = Pipeline([
    ('selector', DataFrameSelector(name_col_num_bin)),
    ('imputer', SimpleImputer(strategy='most_frequent')),
    # non-sparse SciPy matrix with onehot-dense
    ('discretizer', KBinsDiscretizer(n_bins=7, encode='onehot')),
    #('clean_up', VarianceThreshold()),
])

num_freq_pipeline = Pipeline([
    ('selector', DataFrameSelector(name_col_num_bin)),
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('frequency_encoding', FrequencyEncoding()),
])

str_pipeline = Pipeline([
    ('selector_fix_str', FixStringColumns(name_col_str)),
    # check 'max_features' hyperparameter
    ('frequency_encoding_str', FrequencyEncoding()),
    # non-sparse SciPy matrix with sparse=False
    #('one-hot_encoding', OneHotEncoder(handle_unknown='ignore')),
])

full_pipeline = FeatureUnion(transformer_list=[
    ('num_pipeline', num_pipeline),
    ('num_bool_pipeline', num_bool_pipeline),
    #('num_bin_pipeline', num_bin_pipeline),
    ('num_freq_pipeline', num_freq_pipeline),
    ('str_pipeline', str_pipeline),
])


# In[4]:


logging.info("Full pipeline")
t0 = time.perf_counter()
ds_final = full_pipeline.fit_transform(ds)
t1 = time.perf_counter()
t_pipe_ft = t1 - t0
logging.info("Full pipeline, time: %s seconds", t_pipe_ft)
t_pipe_ft


# In[5]:


del ds


# In[6]:


logging.info("Write pipeline pickle")
with open(PIPELINE_PKL, 'wb') as f:
    pickle.dump(full_pipeline, f, pickle.HIGHEST_PROTOCOL)


# In[7]:


logging.info("Transformed data set shape: %s", ds_final.shape)
ds_final.shape


# In[8]:


from sklearn.model_selection import train_test_split


SEED = 25519

def split_data_set(data, labels, test_size=0.2):
    logging.info("Splitting tranformed data set: train %s, test %s",
                 1 - test_size, test_size)
    train_set, test_set, train_labels, test_labels =         train_test_split(data, labels, test_size=test_size, random_state=SEED)

    logging.info("Training set shape %s, test set shape %s; total shape %s",
                 train_set.shape, test_set.shape, data.shape)
    print("Train set shape ", train_set.shape, ", test set shape ",           test_set.shape, "; total shape ", data.shape)

    logging.info("Training labels shape %s, test labels shape %s; total labels shape %s",
                 train_labels.shape, test_labels.shape, labels.shape)
    print("Train labels shape ", train_labels.shape, ", test labels shape ",           test_labels.shape, "; total labels shape ", labels.shape)
    return train_set, test_set, train_labels, test_labels


# In[9]:


train_set_final, test_set_final, train_labels, test_labels = split_data_set(ds_final, ds_labels)


# In[10]:


from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier

from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import GridSearchCV


skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)

if MODEL_NAME == "DecisionTreeClassifier":
    model = DecisionTreeClassifier(random_state=SEED)
    param_grid = [{'max_depth': [11, 13, 15], 'max_features': [62, 64, 66, 68, 70, 72],
                   'max_leaf_nodes': [77, 80, 83]}]
elif MODEL_NAME == "RandomForestClassifier":
    model = RandomForestClassifier(
        max_depth=13, max_features=72, max_leaf_nodes=83,
        random_state=SEED, n_jobs=int(N_CORES / 5)
    )
    param_grid = [{'n_estimators': [110, 120, 130]}]
elif MODEL_NAME == "AdaBoostClassifier":
    model = AdaBoostClassifier(
        base_estimator=DecisionTreeClassifier(
            max_depth=13, max_features=72, max_leaf_nodes=83, random_state=SEED
        ),
        random_state=SEED
    )
    param_grid = [{'learning_rate': [0.7, 1.0], 'n_estimators': [50, 85, 120]}]
elif MODEL_NAME == "GradientBoostingClassifier":
    model = GradientBoostingClassifier(
        loss='deviance', warm_start=True, n_estimators=120,
        max_depth=13, max_features=72, max_leaf_nodes=83,
        random_state=SEED
    )
    param_grid = [{'learning_rate': [0.1, 0.3]}]


# In[11]:


model_gs = GridSearchCV(
    model, param_grid, scoring='roc_auc',
    n_jobs=N_CORES, pre_dispatch=N_CORES,
    refit=True, cv=skf, verbose=101,
    return_train_score=True
)

logging.info("GridSearchCV")
t0 = time.perf_counter()
model_gs.fit(train_set_final, train_labels)
t1 = time.perf_counter()
t_grid = t1 - t0
logging.info("GridSearchCV, time: %s seconds", t_grid)
t_grid


# In[12]:


logging.info("Write GridSearchCV pickle")
with open(MODEL_SEARCH_PKL, 'wb') as f:
    pickle.dump(model_gs, f, pickle.HIGHEST_PROTOCOL)


# In[13]:


logging.info("CV results: %s", model_gs.cv_results_)
model_gs.cv_results_  # time in seconds


# In[14]:


logging.info("Best score: %s", model_gs.best_score_)
model_gs.best_score_


# In[15]:


t_refit = model_gs.refit_time_
logging.info("Refit time with the whole training set: %s seconds", t_refit)
t_refit


# In[16]:


logging.info("Best parameters: %s", model_gs.best_params_)
model_gs.best_params_


# In[17]:


logging.info("Best estimator: %s", model_gs.best_estimator_)
model_gs.best_estimator_


# In[18]:


from sklearn.metrics import accuracy_score, roc_curve, roc_auc_score
import matplotlib
import matplotlib.pyplot as plt

def plot_bar_chart(x_labels, y_0, set_name, y_label, save=False):
    logging.info("Create bar chart plot for %s", set_name)
    x = np.arange(len(x_labels))
    width = 0.3
    fig, ax = plt.subplots()
    train_rect = ax.bar(x, y_0, width, color='green')
    ax.set_ylabel(y_label)
    ax.set_xticks(x)
    ax.set_xticklabels(x_labels)
    plt.ylim(0, 1.09 * max(y_0))
    annotate_rect(ax, train_rect)
    if save is True:
        logging.info("Save bar chart plot for %s", set_name)
        img_name = "bc_{}-{}_{}.png".format(set_name, ROUND, MODEL_NAME)
        img = os.path.join(IMAGES_PATH, img_name)
        plt.savefig(img, dpi=300, transparent=True, bbox_inches='tight')


# In[19]:


def annotate_rect(ax, rectangles):
    for rect in rectangles:
        height = rect.get_height()
        ax.annotate('{0:0.5f}'.format(height),
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 3),
                    textcoords="offset points",
                    ha='center', va='bottom')

def plot_grouped_bar_chart(x_labels, y_0, y_1, set_name, y_label, save=False):
    logging.info("Create grouped bar chart plot for %s", set_name)
    x = np.arange(len(x_labels))
    width = 0.4
    fig, ax = plt.subplots()
    train_rect = ax.bar(x - width / 2, y_0, width, label="Train")
    test_rect = ax.bar(x + width / 2, y_1, width, label="Test")
    ax.set_ylabel(y_label)
    ax.set_xticks(x)
    ax.set_xticklabels(x_labels)
    ax.legend(loc='lower right', framealpha=1)
    plt.ylim(0, 1)
    annotate_rect(ax, train_rect)
    annotate_rect(ax, test_rect)
    if save is True:
        logging.info("Save grouped bar chart plot for %s", set_name)
        img_name = "gbc_{}-{}_{}.png".format(set_name, ROUND, MODEL_NAME)
        img = os.path.join(IMAGES_PATH, img_name)
        plt.savefig(img, dpi=300, transparent=True, bbox_inches='tight')


# In[20]:


def plot_roc_curve(fp, tp, set_name, save=False):
    logging.info("Create ROC plot for %s", set_name)
    plt.plot(fp, tp, label='ROC curve', color='red')
    plt.plot([0,1],[0,1], '--', label='y = x', color='gray')
    plt.gca().set_aspect('equal')
    plt.legend(loc='lower right')
    plt.xlabel('False Positive Rate', fontsize=13)
    plt.ylabel('True Positive Rate', fontsize=13)
    if save is True:
        logging.info("Save ROC plot for %s", set_name)
        img_name = "roc_{}-{}_{}.png".format(set_name, ROUND, MODEL_NAME)
        img = os.path.join(IMAGES_PATH, img_name)
        plt.savefig(img, dpi=300, transparent=True, bbox_inches='tight')


# In[21]:


def predict_roc(model, data, labels, set_name, save=False):
    logging.info("Predict probabilities (%s)", set_name)
    t0 = time.perf_counter()
    predictions = model.predict_proba(data)
    t1 = time.perf_counter()
    t_pp = t1 - t0
    logging.info("Predict probabilities (%s), time: %s seconds", set_name, t_pp)
    roc_auc = roc_auc_score(labels, predictions[:, 1])
    logging.info("ROC AUC (%s): %s", set_name, roc_auc)
    fp, tp, thres = roc_curve(labels, predictions[:, 1])
    plot_roc_curve(fp, tp, set_name, save)
    return t_pp, roc_auc


# In[22]:


def predict_acc(model, data, labels, set_name):
    logging.info("Predict (%s)", set_name)
    t0 = time.perf_counter()
    predictions = model.predict(data)
    t1 = time.perf_counter()
    t_p = t1 - t0
    logging.info("Predict (%s), time: %s seconds", set_name, t_p)
    accuracy = accuracy_score(labels, predictions)
    logging.info("Accuracy (%s): %s", set_name, accuracy)
    return t_p, accuracy


# In[23]:


t_acc_train, acc_train = predict_acc(model_gs, train_set_final, train_labels, "train")
print("Time (in seconds): ", t_acc_train, ", accuracy: ", acc_train)


# In[24]:


t_roc_train, roc_train = predict_roc(model_gs, train_set_final, train_labels, "train", save=True)
print("Time (in seconds): ", t_roc_train, ", ROC AUC: ", roc_train)


# In[25]:


t_acc_test, acc_test = predict_acc(model_gs, test_set_final, test_labels, "test")
print("Time (in seconds): ", t_acc_test, ", accuracy: ", acc_test)


# In[26]:


t_roc_test, roc_test = predict_roc(model_gs, test_set_final, test_labels, "test", save=True)
print("Time (in seconds): ", t_roc_test, ", ROC AUC: ", roc_test)


# In[27]:


# refit with 50% of the trainig set
model_50 = model_gs.best_estimator_

train_set_50, test_set_50, train_labels_50, test_labels_50 =     split_data_set(train_set_final, train_labels, test_size=0.5)

logging.info("Refit with half of the training set")
t0 = time.perf_counter()
model_50.fit(train_set_50, train_labels_50)
t1 = time.perf_counter()
t_refit_50 = t1 - t0
logging.info("Refit with half of the training set, time: %s seconds", t_refit_50)
t_refit_50


# In[28]:


t_acc_train_50, acc_train_50 = predict_acc(model_50, train_set_50, train_labels_50, "train_50")
print("Time (in seconds): ", t_acc_train_50, ", accuracy: ", acc_train_50)


# In[29]:


t_roc_train_50, roc_train_50 = predict_roc(model_50, train_set_50, train_labels_50, "train_50", save=True)
print("Time (in seconds): ", t_roc_train_50, ", ROC AUC: ", roc_train_50)


# In[30]:


t_acc_test_50, acc_test_50 = predict_acc(model_50, test_set_50, test_labels_50, "test_50")
print("Time (in seconds): ", t_acc_test_50, ", accuracy: ", acc_test_50)


# In[31]:


t_roc_test_50, roc_test_50 = predict_roc(model_50, test_set_50, test_labels_50, "test_50", save=True)
print("Time (in seconds): ", t_roc_test_50, ", ROC AUC: ", roc_test_50)


# In[32]:


# refit with 75% of the trainig set
model_75 = model_gs.best_estimator_

train_set_75, test_set_75, train_labels_75, test_labels_75 =     split_data_set(train_set_final, train_labels, test_size=0.25)

logging.info("Refit with 3/4 of the training set")
t0 = time.perf_counter()
model_75.fit(train_set_75, train_labels_75)
t1 = time.perf_counter()
t_refit_75 = t1 - t0
logging.info("Refit with 3/4 of the training set, time: %s seconds", t_refit_75)
t_refit_75


# In[33]:


t_acc_train_75, acc_train_75 = predict_acc(model_75, train_set_75, train_labels_75, "train_75")
print("Time (in seconds): ", t_acc_train_75, ", accuracy: ", acc_train_75)


# In[34]:


t_roc_train_75, roc_train_75 = predict_roc(model_75, train_set_75, train_labels_75, "train_75", save=True)
print("Time (in seconds): ", t_roc_train_75, ", ROC AUC: ", roc_train_75)


# In[35]:


t_acc_test_75, acc_test_75 = predict_acc(model_75, test_set_75, test_labels_75, "test_75")
print("Time (in seconds): ", t_acc_test_75, ", accuracy: ", acc_test_75)


# In[36]:


t_roc_test_75, roc_test_75 = predict_roc(model_75, test_set_75, test_labels_75, "test_75", save=True)
print("Time (in seconds): ", t_roc_test_75, ", ROC AUC: ", roc_test_75)


# In[37]:


# bar chart, fitting times
x_labels = ["50%", "75%", "100%"]
fit_times = [t_refit_50, t_refit_75, t_refit]
plot_bar_chart(x_labels, fit_times, "fit", "Time (s)", save=True)


# In[38]:


# grouped bar chart, accuracy
gbc_acc_train = [acc_train_50, acc_train_75, acc_train]
gbc_acc_test = [acc_test_50, acc_test_75, acc_test]
plot_grouped_bar_chart(x_labels, gbc_acc_train, gbc_acc_test, "accuracy", "Accuracy", save=True)


# In[39]:


# grouped bar chart, roc
gbc_roc_train = [roc_train_50, roc_train_75, roc_train]
gbc_roc_test = [roc_test_50, roc_test_75, roc_test]
plot_grouped_bar_chart(x_labels, gbc_roc_train, gbc_roc_test, "roc", "ROC AUC", save=True)


# In[40]:


# refit with the whole data set
model_final = model_gs.best_estimator_

logging.info("Final refit with the whole data set")
t0 = time.perf_counter()
model_final.fit(ds_final, ds_labels)
t1 = time.perf_counter()
t_refit_final = t1 - t0
logging.info("Final refit with the whole data set, time: %s seconds", t_refit_final)
t_refit_final


# In[41]:


logging.info("Write refitted final model pickle")
with open(MODEL_PKL, 'wb') as f:
    pickle.dump(model_final, f, pickle.HIGHEST_PROTOCOL)


# In[42]:


# save times in a csv file
step_names = [
    "t_load", "t_pipe_ft", "t_grid",
    "t_acc_train_50", "t_acc_train_75", "t_acc_train",
    "t_roc_train_50", "t_roc_train_75", "t_roc_train",
    "t_acc_test_50", "t_acc_test_75", "t_acc_test",
    "t_roc_test_50", "t_roc_test_75", "t_roc_test",
    "t_refit_50", "t_refit_75", "t_refit", "t_refit_final"
]
times_list = [
    t_load, t_pipe_ft, t_grid,
    t_acc_train_50, t_acc_train_75, t_acc_train,
    t_roc_train_50, t_roc_train_75, t_roc_train,
    t_acc_test_50, t_acc_test_75, t_acc_test,
    t_roc_test_50, t_roc_test_75, t_roc_test,
    t_refit_50, t_refit_75, t_refit, t_refit_final
]
times_dict = {'step': step_names, 'time': times_list}
times_df = pd.DataFrame(times_dict)

logging.info("Write training times in a CSV file")
times_csv = os.path.join(
    CSV_PATH,
    "times_training-{}_{}.csv".format(ROUND, MODEL_NAME)
)
times_df.to_csv(times_csv, index=False)


# In[43]:


logging.info("FINISHED %s --- %s, %s", NB, ROUND, MODEL_NAME)

